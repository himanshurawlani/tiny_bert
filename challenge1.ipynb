{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwea3-aF8Uut"
      },
      "source": [
        "# **Universal sentence embeddings background [2 points]**\n",
        "In this takehome, we will be exploring different ways of learning sentence embeddings. Sentence embedding is the collective name for a set of techniques in natural language processing (NLP) where sentences are mapped to vectors of real numbers. For an overview of sentence embeddings and some common methods, we refer these articles: [link1](https://txt.cohere.com/sentence-word-embeddings/), [link2](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)  \n",
        "\n",
        "Q1: What are some real world applications of dense sentence embeddings?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Q2: Apart from using large language models, what are other ways to compute sentence embeddings?\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVPkyE6r8cc9"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4UUlYyX8eh7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from typing import Union, Tuple, List, Iterable, Dict\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "import numpy as np\n",
        "import gzip, csv\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR18Hatm8RDY"
      },
      "outputs": [],
      "source": [
        "%pip install transformers\n",
        "from transformers import AutoTokenizer\n",
        "# If you can not find all the bugs, use the line below for AutoModel\n",
        "#from transformers import AutoModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVtBb1LB8iqA"
      },
      "source": [
        "## **Coding Challenge Part 1: Debugging custom BERT code [8 points]**\n",
        "\n",
        "BERT ([Bidirectional Encoder Representations from Transformers](https://arxiv.org/abs/1810.04805)) is a transformer based language model that is pretrained to generate contextualized embeddings. In this part, we provide a BERT implementation together with a pretrained checkpoint file. This BERT implementation includes 7 bugs in which some of these bugs break the code but some of them only impact the model performance.\n",
        "\n",
        "Tasks:\n",
        "* [**7 points**] Your goal is to get the code working. There are 7 bugs in the code, some of them lead to error in the code but some of them are designed to impair test accuracy but not break the code. You will get one point for each of the 7 bugs you find.\n",
        "\n",
        "* [**1 points**] You will get extra points for also adding improved documentation to each of the functions we introduce in this section, and for describing the fixes to the bugs.\n",
        "\n",
        "\n",
        "Note for usage and comparison:\n",
        "*   In order to test this implementation, we provide ***bert_tiny.bin*** and example usage in the below cells.\n",
        "*   You can check if your bugfixes are correct based on your results in \"Coding challenge Part 2\". Except the Bert imlementation, there is no bugs in other parts, so if your fixes are correct you should achieve the same results. We provide the expected results for you to compare.\n",
        "\n",
        "\n",
        "**Please DO NOT use any additional library except the ones that are imported!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06pAJoSc8gA6"
      },
      "outputs": [],
      "source": [
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class Config(object):\n",
        "    def __init__(self,\n",
        "                vocab_size,\n",
        "                hidden_size=768,\n",
        "                num_hidden_layers=12,\n",
        "                num_attention_heads=12,\n",
        "                intermediate_size=3072,\n",
        "                dropout_prob=0.9,\n",
        "                max_position_embeddings=512,\n",
        "                type_vocab_size=2,\n",
        "                initializer_range=0.02):\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_dropout_prob = dropout_prob\n",
        "        self.attention_probs_dropout_prob = dropout_prob\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.type_vocab_size = type_vocab_size\n",
        "        self.initializer_range = initializer_range\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, dict_object):\n",
        "        config = Config(vocab_size=None)\n",
        "        for (key, value) in dict_object.items():\n",
        "            config.__dict__[key] = value\n",
        "        return config\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "      def __init__(self, hidden_size, variance_epsilon=1e-12):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "      def forward(self, x):\n",
        "        u = x.mean(0, keepdim=True)\n",
        "        s = (x + u).pow(2).mean(0, keepdim=True)\n",
        "        x = (x + u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.gamma * x + self.beta\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "      def __init__(self, hidden_size, intermediate_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.dense_expansion = nn.Linear(hidden_size, intermediate_size)\n",
        "        self.dense_contraction = nn.Linear(intermediate_size, hidden_size)\n",
        "\n",
        "      def forward(self, x):\n",
        "        x = self.dense_expansion(x)\n",
        "        x = self.dense_contraction(gelu(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Layer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Layer, self).__init__()\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "        self.attn_out = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.ln1 = LayerNorm(config.hidden_size)\n",
        "\n",
        "        self.mlp = MLP(config.hidden_size, config.intermediate_size)\n",
        "        self.ln2 = LayerNorm(config.hidden_size)\n",
        "\n",
        "    def split_heads(self, tensor, num_heads, attention_head_size):\n",
        "        new_shape = tensor.size()[:-1] + (num_heads, attention_head_size)\n",
        "        tensor = tensor.view(*new_shape)\n",
        "        return tensor.permute(0, 2, 1, 3)\n",
        "\n",
        "    def merge_heads(self, tensor, num_heads, attention_head_size):\n",
        "        tensor = tensor.permute(0, 2, 1, 3).contiguous()\n",
        "        new_shape = tensor.size()[:-2] + (num_heads * attention_head_size,)\n",
        "        return tensor.view(new_shape)\n",
        "\n",
        "    def attn(self, q, k, v, attention_mask):\n",
        "        mask = attention_mask == 1\n",
        "        mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        s = torch.matmul(q, k)\n",
        "        s = s / math.sqrt(self.attention_head_size)\n",
        "\n",
        "        s = torch.where(mask, s, torch.tensor(float('inf')))\n",
        "\n",
        "        p = s\n",
        "        p = self.dropout(p)\n",
        "\n",
        "        a = torch.matmul(p, v)\n",
        "        return a\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "        q, k, v = self.query(x), self.key(x), self.value(x)\n",
        "\n",
        "        q = self.split_heads(q, self.num_attention_heads, self.attention_head_size)\n",
        "        k = self.split_heads(k, self.num_attention_heads, self.attention_head_size)\n",
        "        v = self.split_heads(v, self.num_attention_heads, self.attention_head_size)\n",
        "\n",
        "        a = self.attn(q, k, v, attention_mask)\n",
        "        a = self.merge_heads(a, self.num_attention_heads, self.attention_head_size)\n",
        "        a = self.attn_out(a)\n",
        "        a = self.dropout(a)\n",
        "        a = self.ln1(a)\n",
        "\n",
        "        m = self.mlp(a)\n",
        "        m = self.dropout(m)\n",
        "        m = self.ln2(m)\n",
        "\n",
        "        return m\n",
        "\n",
        "\n",
        "class Bert(nn.Module):\n",
        "      def __init__(self, config_dict):\n",
        "        super(Bert, self).__init__()\n",
        "        self.config = Config.from_dict(config_dict)\n",
        "        self.embeddings = nn.ModuleDict({\n",
        "          'token': nn.Embedding(self.config.vocab_size, self.config.hidden_size, padding_idx=0),\n",
        "          'position': nn.Embedding(self.config.max_position_embeddings, self.config.hidden_size),\n",
        "          'token_type': nn.Embedding(self.config.type_vocab_size, self.config.hidden_size),\n",
        "        })\n",
        "\n",
        "        self.ln = LayerNorm(self.config.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            Layer(self.config) for _ in range(self.config.num_hidden_layers)\n",
        "        ])\n",
        "\n",
        "        self.pooler = nn.Sequential(OrderedDict([\n",
        "            ('dense', nn.Linear(self.config.hidden_size, self.config.hidden_size)),\n",
        "            ('activation', nn.Tanh()),\n",
        "        ]))\n",
        "\n",
        "      def forward(self, input_ids, attention_mask=None, token_type_ids=None, ):\n",
        "        position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        x = torch.cat((self.embeddings.token(input_ids),\n",
        "                       self.embeddings.position(position_ids),\n",
        "                       self.embeddings.token_type(token_type_ids)),\n",
        "                      dim=-1)\n",
        "        x = self.dropout(self.ln(x))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask)\n",
        "\n",
        "        o = self.pooler(x[:, 0])\n",
        "        return (x, o)\n",
        "\n",
        "      def load_model(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3e6blJo8ugX"
      },
      "source": [
        "**Download weights for the custom Bert**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK_FZGZt8vTm"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/for-ai/bert/raw/master/bert_tiny.bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htg26xoP8zxT"
      },
      "source": [
        "**An example use of pretrained BERT with transformers library to encode a sentence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tAw6W6-80NH"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'prajjwal1/bert-tiny'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "## IF YOU CANNOT SOLVE PREVIOUS BUGS, USE THE LINE BELOW:\n",
        "## bert = AutoModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "bert_config = {\"hidden_size\": 128, \"num_attention_heads\": 2, \"num_hidden_layers\": 2, \"intermediate_size\": 512, \"vocab_size\": 30522}\n",
        "bert = Bert(bert_config).load_model('bert_tiny.bin')\n",
        "\n",
        "#EXAMPLE USE\n",
        "sentence = 'An example use of pretrained BERT with transformers library to encode a sentence'\n",
        "tokenized_sample = tokenizer(sentence, return_tensors='pt', padding='max_length', max_length=512)\n",
        "output = bert(input_ids=tokenized_sample['input_ids'],\n",
        "              attention_mask=tokenized_sample['attention_mask'],)\n",
        "\n",
        "# We use \"pooler_output\" for simplicity. This corresponds the last layer\n",
        "# hidden-state of the first token of the sequence (CLS token) after\n",
        "# further processing through the layers used for the auxiliary pretraining task.\n",
        "embedding = output[1]\n",
        "print(f'\\nResulting embedding shape: {embedding.shape}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
