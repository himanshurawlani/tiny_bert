# Tiny BERT

Implementating tiny BERT from scratch in PyTorch for the C4AI Scholars Program Challenge!

These exercises are designed to allow you to showcase your engineering and problem solving skills. There are four challenges that consists of:

1.  [Coding Challenge Part 1](challenge1.ipynb): Debugging custom BERT code
2.  [Coding Challenge Part 2](challenge2.ipynb): Evaluate a pretrained BERT model on STS benchmark
3.  [Coding Challenge Part 3](challenge3.ipynb): Learning sentence embeddings using Natural Language Inference (NLI) dataset
4.  [Coding Challenge Part 4](challenge4.ipynb): Learning sentence embedding using a contrastive approach based on NLI dataset
5.  Optonal Challenge: Further improve sentence encoder in terms of _performance_ or _efficiency_. You can use any additional model, dataset, library or package for this part!

These tasks were chosen as a setting to see how you think about problems, even if they are not in your own research field of interest. The tasks and dataset are not meant to be indicative of the research goals of the Scholar Program.

## Study Materials

1. Attention is All You Need: https://arxiv.org/abs/1706.03762
2. Andrej Karpathy's Let's build GPT from scratch: https://www.youtube.com/watch?v=kCc8FmEb1nY
3. The Illustrated Transformer: http://jalammar.github.io/illustrated-transformer/
4. Huggingface's BERT implementation: https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py
5. Sentence Transformers STS Training: https://www.sbert.net/examples/training/sts/README.html
6. Sentence Transformers NLI Training: https://www.sbert.net/examples/training/nli/README.html
7. Contrastive loss: https://www.sbert.net/examples/training/quora_duplicate_questions/README.html#constrative-loss
8. MultipleNegativesRanking Loss: https://www.sbert.net/examples/training/nli/README.html#multiplenegativesrankingloss

Good luck!
